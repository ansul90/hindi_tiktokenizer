"""
Train Hindi BPE Tokenizer - Simple Version
"""

from hindi_bpe_tokenizer import HindiBPETokenizer
import json

# Load corpus
print("=" * 80)
print("Hindi BPE Tokenizer Training")
print("=" * 80)
print()

print("Loading corpus...")
with open('hindi_corpus.txt', 'r', encoding='utf-8') as f:
    corpus = f.read()

print(f"âœ“ Corpus loaded")
print(f"  Total characters: {len(corpus):,}")
print(f"  Total bytes (UTF-8): {len(corpus.encode('utf-8')):,}")

# Train tokenizer
print("\n" + "=" * 80)
print("Training BPE Tokenizer...")
print("=" * 80)

vocab_size = 5500  # Increase target to ensure we hit 5000+
tokenizer = HindiBPETokenizer(vocab_size=vocab_size)
tokenizer.train(corpus, verbose=True)

# Save tokenizer
tokenizer.save('hindi_bpe_tokenizer.json')

# Test on various Hindi texts
print("\n" + "=" * 80)
print("Testing the Tokenizer...")
print("=" * 80)

test_texts = [
    "à¤­à¤¾à¤°à¤¤à¥€à¤¯ à¤…à¤‚à¤¤à¤°à¤¿à¤•à¥à¤· à¤…à¤¨à¥à¤¸à¤‚à¤§à¤¾à¤¨ à¤¸à¤‚à¤—à¤ à¤¨ à¤¨à¥‡ à¤šà¤‚à¤¦à¥à¤°à¤¯à¤¾à¤¨-3 à¤®à¤¿à¤¶à¤¨ à¤•à¥‹ à¤¸à¤«à¤²à¤¤à¤¾à¤ªà¥‚à¤°à¥à¤µà¤• à¤²à¥‰à¤¨à¥à¤š à¤•à¤¿à¤¯à¤¾à¥¤",
    "à¤¯à¤¹ à¤à¤• à¤¬à¤¹à¥à¤¤ à¤¹à¥€ à¤®à¤¹à¤¤à¥à¤µà¤ªà¥‚à¤°à¥à¤£ à¤‰à¤ªà¤²à¤¬à¥à¤§à¤¿ à¤¹à¥ˆ à¤œà¥‹ à¤­à¤¾à¤°à¤¤ à¤•à¥‡ à¤…à¤‚à¤¤à¤°à¤¿à¤•à¥à¤· à¤•à¤¾à¤°à¥à¤¯à¤•à¥à¤°à¤® à¤•à¥‡ à¤²à¤¿à¤ à¤à¤• à¤®à¥€à¤² à¤•à¤¾ à¤ªà¤¤à¥à¤¥à¤° à¤¹à¥ˆà¥¤",
    "à¤µà¤¿à¤œà¥à¤žà¤¾à¤¨ à¤”à¤° à¤ªà¥à¤°à¥Œà¤¦à¥à¤¯à¥‹à¤—à¤¿à¤•à¥€ à¤•à¥‡ à¤•à¥à¤·à¥‡à¤¤à¥à¤° à¤®à¥‡à¤‚ à¤­à¤¾à¤°à¤¤ à¤¨à¥‡ à¤‰à¤²à¥à¤²à¥‡à¤–à¤¨à¥€à¤¯ à¤ªà¥à¤°à¤—à¤¤à¤¿ à¤•à¥€ à¤¹à¥ˆà¥¤",
    "à¤¹à¤¿à¤‚à¤¦à¥€ à¤­à¤¾à¤·à¤¾ à¤µà¤¿à¤¶à¥à¤µ à¤•à¥€ à¤ªà¥à¤°à¤®à¥à¤– à¤­à¤¾à¤·à¤¾à¤“à¤‚ à¤®à¥‡à¤‚ à¤¸à¥‡ à¤à¤• à¤¹à¥ˆà¥¤",
    "à¤­à¤¾à¤°à¤¤ à¤¨à¥‡ à¤šà¥Œà¤¥à¥‡ à¤Ÿà¥€-20 à¤®à¥ˆà¤š à¤®à¥‡à¤‚ à¤‘à¤¸à¥à¤Ÿà¥à¤°à¥‡à¤²à¤¿à¤¯à¤¾ à¤•à¥‹ 48 à¤°à¤¨ à¤¸à¥‡ à¤¹à¤°à¤¾ à¤¦à¤¿à¤¯à¤¾à¥¤",
    "à¤µà¥‰à¤¶à¤¿à¤‚à¤—à¤Ÿà¤¨ à¤¸à¥à¤‚à¤¦à¤° à¤¨à¥‡ 3 à¤°à¤¨ à¤¦à¥‡à¤•à¤° 3 à¤µà¤¿à¤•à¥‡à¤Ÿ à¤à¤Ÿà¤•à¥‡à¥¤",
    "à¤¶à¥à¤­à¤®à¤¨ à¤—à¤¿à¤² à¤¨à¥‡ à¤¸à¤¬à¤¸à¥‡ à¤œà¥à¤¯à¤¾à¤¦à¤¾ 46 à¤°à¤¨à¥‹à¤‚ à¤•à¥€ à¤ªà¤¾à¤°à¥€ à¤–à¥‡à¤²à¥€à¥¤",
    "à¤œà¤¸à¤ªà¥à¤°à¥€à¤¤ à¤¬à¥à¤®à¤°à¤¾à¤¹ à¤•à¥‡ 99 à¤µà¤¿à¤•à¥‡à¤Ÿ à¤¹à¥‹ à¤—à¤ à¤¹à¥ˆà¤‚à¥¤"
]

total_original_bytes = 0
total_compressed_tokens = 0

for i, text in enumerate(test_texts, 1):
    print(f"\nTest {i}:")
    print(f"Text: {text[:60]}..." if len(text) > 60 else f"Text: {text}")
    
    stats = tokenizer.get_compression_stats(text)
    encoded = tokenizer.encode(text)
    decoded = tokenizer.decode(encoded)
    
    print(f"  Original bytes: {stats['original_bytes']}")
    print(f"  Compressed tokens: {stats['compressed_tokens']}")
    print(f"  Compression ratio: {stats['compression_ratio']:.2f}X")
    print(f"  Decoding matches: {'âœ“' if text == decoded else 'âœ—'}")
    
    total_original_bytes += stats['original_bytes']
    total_compressed_tokens += stats['compressed_tokens']

overall_compression = total_original_bytes / total_compressed_tokens if total_compressed_tokens > 0 else 0

print("\n" + "=" * 80)
print("Final Results:")
print("=" * 80)
print(f"âœ“ Vocabulary size: {len(tokenizer.vocab):,} tokens")
print(f"âœ“ Number of merges: {len(tokenizer.merges):,}")
print(f"âœ“ Overall test compression ratio: {overall_compression:.2f}X")

# Check if requirements are met
print("\n" + "=" * 80)
print("Requirements Check:")
print("=" * 80)

vocab_ok = len(tokenizer.vocab) >= 5000
compression_ok = overall_compression >= 3.0

print(f"{'âœ“' if vocab_ok else 'âœ—'} Vocabulary size >= 5000: {vocab_ok} ({len(tokenizer.vocab):,} tokens)")
print(f"{'âœ“' if compression_ok else 'âœ—'} Compression ratio >= 3.0: {compression_ok} ({overall_compression:.2f}X)")

if vocab_ok and compression_ok:
    print("\nðŸŽ‰ SUCCESS! All requirements met!")
else:
    print("\nâš ï¸  Some requirements not met.")

# Save detailed statistics
results = {
    'vocab_size': len(tokenizer.vocab),
    'num_merges': len(tokenizer.merges),
    'compression_ratio': overall_compression,
    'corpus_size_bytes': len(corpus.encode('utf-8')),
    'corpus_size_chars': len(corpus),
    'requirements_met': {
        'vocab_size_5000+': vocab_ok,
        'compression_3+': compression_ok
    },
    'test_results': [
        {
            'text': text[:50] + '...' if len(text) > 50 else text,
            'original_bytes': tokenizer.get_compression_stats(text)['original_bytes'],
            'compressed_tokens': tokenizer.get_compression_stats(text)['compressed_tokens'],
            'compression_ratio': tokenizer.get_compression_stats(text)['compression_ratio']
        }
        for text in test_texts
    ]
}

with open('training_results.json', 'w', encoding='utf-8') as f:
    json.dump(results, f, indent=2, ensure_ascii=False)
print(f"\nâœ“ Results saved to training_results.json")

